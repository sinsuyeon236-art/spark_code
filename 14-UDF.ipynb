{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9c3480",
   "metadata": {},
   "source": [
    "# UDF(User Defined Function)\n",
    "- 사용자가 정의 하는 (데이터 변환)함수\n",
    "- 데이터프레임에서 사용이 가능. SQL에서도 사용이 가능!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46efef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/29 05:38:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"udf\").getOrCreate()\n",
    "\n",
    "transactions = [\n",
    "    ('찹쌀탕수육+짜장2', '2021-11-07 13:20:00', 22000, 'KRW'),\n",
    "    ('등심탕수육+크립새우+짜장면', '2021-10-24 11:19:00', 21500, 'KRW'), \n",
    "    ('월남 쌈 2인 세트', '2021-07-25 11:12:40', 42000, 'KRW'), \n",
    "    ('콩국수+열무비빔국수', '2021-07-10 08:20:00', 21250, 'KRW'), \n",
    "    ('장어소금+고추장구이', '2021-07-01 05:36:00', 68700, 'KRW'), \n",
    "    ('족발', '2020-08-19 19:04:00', 32000, 'KRW'),  \n",
    "]\n",
    "\n",
    "schema = [\"name\", \"datetime\", \"price\", \"currency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e8c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=transactions, schema=schema)\n",
    "df.createOrReplaceTempView(\"transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507debe1",
   "metadata": {},
   "source": [
    "UDF를 만든다\n",
    "- 분산 병렬 처리 환경에서 사용할 수 있는 함수 만들기(Worker에서 작동하는 함수)\n",
    "- 리턴 타입을 따로 지정 하지 않으면 기본적으로 String 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b741feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------------------+-----+--------+\n",
      "|                      name|           datetime|price|currency|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "|          찹쌀탕수육+짜장2|2021-11-07 13:20:00|22000|     KRW|\n",
      "|등심탕수육+크립새우+짜장면|2021-10-24 11:19:00|21500|     KRW|\n",
      "|          월남 쌈 2인 세트|2021-07-25 11:12:40|42000|     KRW|\n",
      "|       콩국수+열무비빔국수|2021-07-10 08:20:00|21250|     KRW|\n",
      "|       장어소금+고추장구이|2021-07-01 05:36:00|68700|     KRW|\n",
      "|                      족발|2020-08-19 19:04:00|32000|     KRW|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c859a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def squared(n):\n",
    "    return n * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4feb3ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.squared(n)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register(\"Worker에서 사용할 이름(SQL)\", 마스터에 정의된 함수 이름, 리턴 타입)\n",
    "spark.udf.register(\"squared\", squared, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4020a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|price|squared(price)|\n",
      "+-----+--------------+\n",
      "|22000|     484000000|\n",
      "|21500|     462250000|\n",
      "|42000|    1764000000|\n",
      "|21250|     451562500|\n",
      "|68700|    4719690000|\n",
      "|32000|    1024000000|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT price, squared(price)\n",
    "FROM transactions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04d0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_number(n):\n",
    "    units = [\"\", \"십\", \"백\", \"천\", \"만\"]\n",
    "    nums = '일이삼사오육칠팔구'\n",
    "    result = []\n",
    "    i = 0\n",
    "    while n > 0:\n",
    "        n, r = divmod(n, 10)\n",
    "        if r > 0:\n",
    "            result.append(nums[r-1]+units[i])\n",
    "        i += 1\n",
    "    return \"\".join(reversed(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291a9621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼만오천'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_number(35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c97b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.read_number(n)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"read_number\", read_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51dc1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|price|read_number(price)|\n",
      "+-----+------------------+\n",
      "|22000|          이만이천|\n",
      "|21500|      이만일천오백|\n",
      "|42000|          사만이천|\n",
      "|21250|  이만일천이백오십|\n",
      "|68700|      육만팔천칠백|\n",
      "|32000|          삼만이천|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT price, read_number(price)\n",
    "FROM transactions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "429f335c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_weekday(date)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weekday(date):\n",
    "    import calendar\n",
    "    return calendar.day_name[date.weekday()]\n",
    "\n",
    "spark.udf.register(\"get_weekday\", get_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4af5b80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|           datetime|day_of_week|\n",
      "+-------------------+-----------+\n",
      "|2021-11-07 13:20:00|     Sunday|\n",
      "|2021-10-24 11:19:00|     Sunday|\n",
      "|2021-07-25 11:12:40|     Sunday|\n",
      "|2021-07-10 08:20:00|   Saturday|\n",
      "|2021-07-01 05:36:00|   Thursday|\n",
      "|2020-08-19 19:04:00|  Wednesday|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT datetime, get_weekday(TO_DATE(datetime)) as day_of_week\n",
    "FROM transactions\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9899df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/home/ubuntu/working/spark-examples/data/titanic_train.csv\"\n",
    "titanic_sdf = spark.read.csv(filepath, inferSchema=True, header=True)\n",
    "\n",
    "titanic_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a28c82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(age):\n",
    "    cat = ''\n",
    "    \n",
    "    if age <= 5: cat = 'Baby'\n",
    "    elif age <= 12: cat = 'Child'\n",
    "    elif age <= 18: cat = 'Teenager'\n",
    "    elif age <= 25: cat = 'Student'\n",
    "    elif age <= 35: cat = 'Young Adult'\n",
    "    elif age <= 60: cat = 'Adult'\n",
    "    else : cat = 'Elderly'\n",
    "    \n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f604ef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "udf_get_category = udf(lambda x : get_category(x), StringType())\n",
    "udf_get_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe738b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.69911764705882"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Age NaN값 처리\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "avg_age = titanic_sdf.select(F.avg(F.col(\"Age\")))\n",
    "avg_age_row = avg_age.first()\n",
    "avg_age_value = avg_age_row[0]\n",
    "avg_age_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58a1fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sdf = titanic_sdf.fillna(value=avg_age_value, subset=[\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6efd5d99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+-----------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|              Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|AgeCategory|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+-----------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|             22.0|    1|    0|       A/5 21171|   7.25| null|       S|    Student|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|             38.0|    1|    0|        PC 17599|71.2833|  C85|       C|      Adult|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|             26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|Young Adult|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|             35.0|    1|    0|          113803|   53.1| C123|       S|Young Adult|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|             35.0|    0|    0|          373450|   8.05| null|       S|Young Adult|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|29.69911764705882|    0|    0|          330877| 8.4583| null|       Q|Young Adult|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|             54.0|    0|    0|           17463|51.8625|  E46|       S|      Adult|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|              2.0|    3|    1|          349909| 21.075| null|       S|       Baby|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|             27.0|    0|    2|          347742|11.1333| null|       S|Young Adult|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|             14.0|    1|    0|          237736|30.0708| null|       C|   Teenager|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|              4.0|    1|    1|         PP 9549|   16.7|   G6|       S|       Baby|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|             58.0|    0|    0|          113783|  26.55| C103|       S|      Adult|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|             20.0|    0|    0|       A/5. 2151|   8.05| null|       S|    Student|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|             39.0|    1|    5|          347082| 31.275| null|       S|      Adult|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|             14.0|    0|    0|          350406| 7.8542| null|       S|   Teenager|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|             55.0|    0|    0|          248706|   16.0| null|       S|      Adult|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|              2.0|    4|    1|          382652| 29.125| null|       Q|       Baby|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|29.69911764705882|    0|    0|          244373|   13.0| null|       S|Young Adult|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|             31.0|    1|    0|          345763|   18.0| null|       S|Young Adult|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|29.69911764705882|    0|    0|            2649|  7.225| null|       C|Young Adult|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_sdf.withColumn(\"AgeCategory\", udf_get_category(col(\"Age\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77c4e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
